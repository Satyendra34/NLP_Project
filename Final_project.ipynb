{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11608025,"sourceType":"datasetVersion","datasetId":7280920}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install required libraries with compatible versions\n!pip install torch==2.5.1 transformers==4.44.2 tokenizers==0.19.1 peft==0.12.0 datasets==2.21.0 sentencepiece==0.2.0 accelerate==0.34.2 nltk==3.9.1 rouge_score==0.1.2 bert-score==0.3.13 sacrebleu==2.4.3 numpy>=1.26.4 fsspec>=2024.6.1 spacy==3.7.6 --force-reinstall --no-cache-dir --quiet\n\n# Set environment variables\nimport os\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n# Import libraries\nimport json\nimport numpy as np\nimport torch\nimport nltk\nimport spacy\nfrom datasets import load_dataset\nfrom rouge_score import rouge_scorer\nfrom sacrebleu.metrics import BLEU\nfrom bert_score import score as bert_score\nfrom transformers import (\n    AutoConfig,\n    AutoModelForSeq2SeqLM,\n    AutoTokenizer,\n    DataCollatorForSeq2Seq,\n    Seq2SeqTrainer,\n    Seq2SeqTrainingArguments,\n    set_seed,\n    GenerationConfig,\n    __version__ as transformers_version\n)\nfrom peft import LoraConfig, get_peft_model, TaskType\nnltk.download(\"punkt\", quiet=True)\nnlp = spacy.load(\"en_core_web_sm\")\n\n# Verify library versions\nprint(f\"Installed transformers version: {transformers_version}\")\n\n# Set random seed for reproducibility\ndef seed_everything(seed=42):\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    set_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.backends.cudnn.deterministic = True\nseed_everything()\n\n# Initialize model and tokenizer\nmodel_name = \"t5-base\"\nconfig = AutoConfig.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name, model_max_length=512)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name, config=config)\nmodel.resize_token_embeddings(len(tokenizer))\n\n# Enable gradient checkpointing\nmodel.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": False})\n\n# Apply LoRA\nlora_config = LoraConfig(\n    task_type=TaskType.SEQ_2_SEQ_LM,\n    r=8,\n    lora_alpha=32,\n    lora_dropout=0.1,\n    target_modules=[\"q\", \"v\"]\n)\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()\n\n# Debug dataset\nprint(\"Available datasets in /kaggle/input/:\")\nprint(os.listdir('/kaggle/input/'))\nprint(\"Files in /kaggle/input/feta-dataset/:\")\nprint(os.listdir('/kaggle/input/feta-dataset/'))\nwith open('/kaggle/input/feta-dataset/fetaQA-v1_train.jsonl', 'r') as f:\n    print(\"First 3 lines of fetaQA-v1_train.jsonl:\")\n    for i, line in enumerate(f):\n        if i < 3:\n            try:\n                parsed = json.loads(line.strip())\n                print(f\"Line {i+1} (parsed):\", parsed)\n            except json.JSONDecodeError as e:\n                print(f\"Line {i+1} (raw, failed to parse):\", line.strip(), f\"Error: {e}\")\n        else:\n            break\n\n# Load FeTaQA dataset\ndataset = load_dataset('json', data_files={\n    'train': '/kaggle/input/feta-dataset/fetaQA-v1_train.jsonl',\n    'valid': '/kaggle/input/feta-dataset/fetaQA-v1_dev.jsonl',\n    'test': '/kaggle/input/feta-dataset/fetaQA-v1_test.jsonl'\n})\nprint(\"Sample train example:\", dataset['train'][0])\n\n# Question Parsing Module\ndef parse_question(question):\n    doc = nlp(question)\n    keywords = [token.text.lower() for token in doc if token.pos_ in [\"NOUN\", \"PROPN\", \"NUM\"]]\n    intent = \"aggregate\" if any(word in question.lower() for word in [\"total\", \"sum\", \"average\", \"count\"]) else \"lookup\"\n    return {\"keywords\": keywords, \"intent\": intent}\n\n# Information Retrieval Module\ndef retrieve_relevant_cells(example):\n    table_array = example['table_array']\n    question = example['question']\n    highlighted_cells = example['highlighted_cell_ids']\n    parsed = parse_question(question)\n    keywords = parsed[\"keywords\"]\n    intent = parsed[\"intent\"]\n    \n    header = table_array[0]\n    rows = table_array[1:]\n    \n    relevant_cols = [i for i, col in enumerate(header) if any(kw.lower() in str(col).lower() for kw in keywords)]\n    if not relevant_cols:\n        relevant_cols = [i for i, _ in enumerate(header)]\n    \n    relevant_rows = set()\n    for cell_id in highlighted_cells:\n        if cell_id[0] > 0:\n            relevant_rows.add(cell_id[0] - 1)\n    for i, row in enumerate(rows):\n        if any(any(kw.lower() in str(cell).lower() for kw in keywords) for cell in row):\n            relevant_rows.add(i)\n    \n    relevant_cells = [[row_idx + 1, col_idx] for row_idx in relevant_rows for col_idx in relevant_cols]\n    return relevant_cells, intent\n\n# Reasoning Module\ndef reason_over_cells(table_array, relevant_cells, intent):\n    if intent == \"aggregate\":\n        values = []\n        for cell in relevant_cells:\n            row, col = cell\n            try:\n                value = float(table_array[row][col])\n                values.append(value)\n            except (ValueError, TypeError):\n                continue\n        if values:\n            return f\"Aggregated value: {sum(values) / len(values) if 'average' in intent else sum(values)}\"\n    return None\n\n# Enhanced Table Linearization\ndef linearize_table_context(example):\n    table_array = example['table_array']\n    question = example['question']\n    highlighted_cells = example['highlighted_cell_ids']\n    parsed = parse_question(question)\n    intent = parsed[\"intent\"]\n    header = table_array[0]\n    rows = table_array[1:]\n    \n    # List highlighted cells explicitly\n    highlighted_str = \" [HIGHLIGHTED_CELLS] \"\n    for cell_id in highlighted_cells:\n        row_idx, col_idx = cell_id\n        if row_idx == 0:  # Header cell\n            cell_value = header[col_idx]\n        else:  # Data cell\n            cell_value = rows[row_idx - 1][col_idx]\n        highlighted_str += f\"row {row_idx} column {col_idx}: {cell_value}, \"\n    highlighted_str = highlighted_str.rstrip(\", \")  # Remove trailing comma and space\n    \n    # Linearize table with column names\n    table_str = \" [TABLE] [HEADER] \" + \" | \".join(str(col) for col in header) + \" [ROWS] \"\n    for i, row in enumerate(rows):\n        row_str = \"[ROW] \"\n        for j, cell in enumerate(row):\n            col_name = header[j]\n            row_str += f\"{col_name}: {cell} [SEP] \"\n        row_str = row_str.rstrip(\" [SEP] \")  # Remove trailing separator\n        table_str += row_str + \" \"\n    \n    # Combine question, intent, highlighted cells, and table\n    linearized = f\"[QUESTION] {question} [INTENT] {intent}{highlighted_str}{table_str}\"\n    \n    # Add reasoning for aggregate intents\n    if intent == \"aggregate\":\n        relevant_cells, _ = retrieve_relevant_cells(example)\n        reasoning_output = reason_over_cells(table_array, relevant_cells, intent)\n        if reasoning_output:\n            linearized += f\" [REASONING] {reasoning_output}\"\n    \n    return linearized\n\n# Preprocess dataset\ndef preprocess_examples(examples):\n    prefix = 'answer: '\n    inputs = [prefix + linearize_table_context(example) for example in examples]\n    answers = [example['answer'] for example in examples]\n    \n    model_inputs = tokenizer(\n        inputs,\n        max_length=512,\n        padding=\"max_length\",\n        truncation=True,\n        return_tensors=\"pt\"\n    )\n    \n    labels = tokenizer(\n        answers,\n        max_length=64,\n        padding=\"max_length\",\n        truncation=True,\n        return_tensors=\"pt\"\n    ).input_ids\n    \n    labels = [[-100 if token == tokenizer.pad_token_id else token for token in label] for label in labels]\n    model_inputs[\"labels\"] = labels\n    return model_inputs\n\n# Apply preprocessing\ndef process_batch(batch):\n    keys = batch.keys()\n    batch = [{k: batch[k][i] for k in keys} for i in range(len(batch[list(keys)[0]]))]\n    return preprocess_examples(batch)\n\nencoded_train_ds = dataset['train'].map(process_batch, batched=True, remove_columns=dataset['train'].column_names)\nencoded_val_ds = dataset['valid'].map(process_batch, batched=True, remove_columns=dataset['valid'].column_names)\nencoded_test_ds = dataset['test'].map(process_batch, batched=True, remove_columns=dataset['test'].column_names)\n\n# Post-process text for evaluation\ndef postprocess_text(preds, labels, metric_name):\n    preds = [pred.strip() for pred in preds]\n    labels = [label.strip() for label in labels]\n    if metric_name == \"rouge\":\n        preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n        labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n    elif metric_name == \"sacrebleu\":\n        labels = [[label] for label in labels]\n    return preds, labels\n\n# Compute evaluation metrics manually\ndef compute_metrics(eval_preds):\n    preds, labels = eval_preds\n    if isinstance(preds, tuple):\n        preds = preds[0]\n    \n    # Clip token IDs to valid range\n    preds = np.clip(preds, 0, tokenizer.vocab_size - 1)\n    \n    # Decode predictions\n    decoded_preds = []\n    for pred in preds:\n        try:\n            decoded = tokenizer.decode(pred, skip_special_tokens=True)\n            decoded_preds.append(decoded if decoded else \"<empty>\")\n        except Exception as e:\n            print(f\"Error decoding prediction: {e}\")\n            decoded_preds.append(\"<error>\")\n    \n    # Decode labels\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = [tokenizer.decode(label, skip_special_tokens=True) for label in labels]\n    \n    # Save predictions and labels for debugging\n    save_path = os.path.join(training_args.output_dir, \"predictions.json\")\n    with open(save_path, \"w\") as f:\n        json.dump({\"predictions\": decoded_preds, \"labels\": decoded_labels}, f, indent=4)\n    \n    result = {}\n    \n    # SacreBLEU\n    try:\n        bleu = BLEU()\n        preds_bleu, labels_bleu = postprocess_text(decoded_preds, decoded_labels, \"sacrebleu\")\n        bleu_score = bleu.corpus_score(preds_bleu, labels_bleu).score\n        result[\"sacrebleu\"] = round(bleu_score, 4)\n    except Exception as e:\n        print(f\"Error computing sacrebleu: {e}\")\n        result[\"sacrebleu\"] = 0.0\n    \n    # ROUGE\n    try:\n        scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n        preds_rouge, labels_rouge = postprocess_text(decoded_preds, decoded_labels, \"rouge\")\n        rouge_scores = {\"rouge1\": 0.0, \"rouge2\": 0.0, \"rougeL\": 0.0}\n        for pred, label in zip(preds_rouge, labels_rouge):\n            scores = scorer.score(label, pred)\n            for key in rouge_scores:\n                rouge_scores[key] += scores[key].fmeasure\n        for key in rouge_scores:\n            rouge_scores[key] /= len(preds_rouge)\n            result[key] = round(rouge_scores[key], 4)\n    except Exception as e:\n        print(f\"Error computing rouge: {e}\")\n        result.update({\"rouge1\": 0.0, \"rouge2\": 0.0, \"rougeL\": 0.0})\n    \n    # BERTScore\n    try:\n        P, R, F1 = bert_score(decoded_preds, decoded_labels, lang=\"en\", model_type=\"roberta-base\", verbose=False)\n        result.update({\n            \"bertscore_precision\": round(P.mean().item(), 4),\n            \"bertscore_recall\": round(R.mean().item(), 4),\n            \"bertscore_f1\": round(F1.mean().item(), 4)\n        })\n    except Exception as e:\n        print(f\"Error computing bertscore: {e}\")\n        result.update({\"bertscore_precision\": 0.0, \"bertscore_recall\": 0.0, \"bertscore_f1\": 0.0})\n    \n    return result\n\n# Training arguments\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"/kaggle/working/\",\n    overwrite_output_dir=True,\n    do_train=True,\n    do_eval=True,\n    do_predict=True,\n    num_train_epochs=10,\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    gradient_accumulation_steps=8,\n    warmup_steps=500,\n    weight_decay=0.01,\n    learning_rate=3e-5,\n    optim=\"adamw_torch\",\n    predict_with_generate=True,\n    generation_max_length=64,\n    generation_num_beams=4,\n    eval_strategy=\"steps\",\n    eval_steps=1000,\n    save_strategy=\"steps\",\n    save_steps=1000,\n    fp16=torch.cuda.is_available(),\n    logging_steps=500,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"bertscore_f1\",\n    report_to=\"none\",\n    gradient_checkpointing=True,\n    gradient_checkpointing_kwargs={\"use_reentrant\": False}\n)\n\n# Initialize data collator\ndata_collator = DataCollatorForSeq2Seq(\n    tokenizer,\n    model=model,\n    label_pad_token_id=-100\n)\n\n# Initialize trainer\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=encoded_train_ds,\n    eval_dataset=encoded_val_ds,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics\n)\n\n# Train the model\ntrainer.train()\n\n# Evaluate on test set\ntest_results = trainer.predict(encoded_test_ds)\nprint(\"Test Results:\", test_results.metrics)\n\n# Save the model\ntrainer.save_model(\"/kaggle/working/tableqa_model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T11:37:54.669310Z","iopub.execute_input":"2025-04-30T11:37:54.669641Z","iopub.status.idle":"2025-04-30T15:32:41.711867Z","shell.execute_reply.started":"2025-04-30T11:37:54.669618Z","shell.execute_reply":"2025-04-30T15:32:41.710949Z"}},"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: The candidate selected for download or install is a yanked version: 'spacy' candidate (version 3.7.6 at https://files.pythonhosted.org/packages/89/70/9a54469cf5263d4e4079b329458492a1e150810587b7c82961bee208cfa5/spacy-3.7.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from https://pypi.org/simple/spacy/) (requires-python:>=3.7))\nReason for being yanked: Incorrect compatibility for transformer models\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 20.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 20.0.0 which is incompatible.\nsigstore 3.6.1 requires rich~=13.0, but you have rich 14.0.0 which is incompatible.\nydata-profiling 4.16.1 requires matplotlib<=3.10,>=3.5, but you have matplotlib 3.10.1 which is incompatible.\nnilearn 0.11.1 requires scikit-learn>=1.4.0, but you have scikit-learn 1.2.2 which is incompatible.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.9.1 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.5, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\ngensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.15.2 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\ntensorflow-metadata 1.16.1 requires protobuf<6.0.0dev,>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\ngoogle-spark-connect 0.5.2 requires google-api-core>=2.19.1, but you have google-api-core 1.34.1 which is incompatible.\npandas-gbq 0.26.1 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.6.1 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 44.0.2 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.0.0 which is incompatible.\nimbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\nibis-framework 9.2.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nibis-framework 9.2.0 requires toolz<1,>=0.11, but you have toolz 1.0.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\ngoogle-cloud-bigtable 2.28.1 requires google-api-core[grpc]<3.0.0dev,>=2.16.0, but you have google-api-core 1.34.1 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\nlangchain-core 0.3.35 requires packaging<25,>=23.2, but you have packaging 25.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"},{"name":"stderr","text":"2025-04-30 11:40:49.815513: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746013250.004824      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746013250.063010      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Installed transformers version: 4.44.2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44e1d2d8b5324b5ebcb8775cfa11fba6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1af12195f36d4fcdb9ab3964b86ed945"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54a0556246f54d80bf271dab5cea1e17"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\nXet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b6847313edf44af8d53cc8f243b2f01"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ba9e9e245cc4f49b2c40358ae069047"}},"metadata":{}},{"name":"stdout","text":"trainable params: 884,736 || all params: 223,766,784 || trainable%: 0.3954\nAvailable datasets in /kaggle/input/:\n['feta-dataset']\nFiles in /kaggle/input/feta-dataset/:\n['fetaQA-v1_train.jsonl', 'fetaQA-v1_dev.jsonl', 'fetaQA-v1_test.jsonl']\nFirst 3 lines of fetaQA-v1_train.jsonl:\nLine 1 (parsed): {'feta_id': 18162, 'table_source_json': 'totto_source/train_json/example-10461.json', 'page_wikipedia_url': 'http://en.wikipedia.org/wiki/1982_Illinois_gubernatorial_election', 'table_page_title': '1982 Illinois gubernatorial election', 'table_section_title': 'Results', 'table_array': [['Party', 'Party', 'Candidate', 'Votes', '%', '±'], ['-', 'Republican', 'James R. Thompson (incumbent)', '1,816,101', '49.44', '-'], ['-', 'Democratic', 'Adlai Stevenson III', '1,811,027', '49.30', '-'], ['-', 'Libertarian', 'Bea Armstrong', '24,417', '0.66', '-'], ['-', 'Taxpayers', 'John E. Roche', '22,001', '0.60', '-'], ['-', 'N/A', 'write-ins', '161', '0.00', 'n-a'], ['Majority', 'Majority', 'Majority', '5,074', '0.14', '-'], ['Turnout', 'Turnout', 'Turnout', '3,673,707', '-', '-'], ['-', 'Republican hold', 'Republican hold', 'Swing', '-', '-']], 'highlighted_cell_ids': [[1, 2], [6, 3]], 'question': 'Who won the 1982 Illinois gubernatorial election, and how many votes was the margin?', 'answer': 'Thompson prevailed in the 1982 Illinois gubernatorial election by a 5,074 vote margin.'}\nLine 2 (parsed): {'feta_id': 11292, 'table_source_json': 'totto_source/train_json/example-3591.json', 'page_wikipedia_url': 'http://en.wikipedia.org/wiki/1986_Indianapolis_500', 'table_page_title': '1986 Indianapolis 500', 'table_section_title': 'Race box score', 'table_array': [['Finish', 'Start', 'No', 'Name', 'Qual', 'Laps', 'Status'], ['1', '4', '3', 'United States Bobby Rahal', '213.550', '200', '170.722 mph'], ['2', '6', '7', 'United States Kevin Cogan', '211.922', '200', '+1.441 seconds'], ['3', '1', '4', 'United States Rick Mears (W)', '216.828', '200', '+1.881 seconds'], ['4', '8', '5', 'Colombia Roberto Guerrero', '211.576', '200', '+10.558 seconds'], ['5', '9', '30', 'United States Al Unser, Jr.', '211.533', '199', 'Flagged'], ['6', '3', '18', 'United States Michael Andretti', '214.522', '199', 'Flagged'], ['7', '11', '20', 'Brazil Emerson Fittipaldi', '210.237', '199', 'Flagged'], ['8', '12', '21', 'United States Johnny Rutherford (W)', '210.220', '198', 'Flagged'], ['9', '2', '1', 'United States Danny Sullivan (W)', '215.382', '197', 'Flagged'], ['10', '13', '12', 'United States Randy Lanier (R)', '209.964', '195', 'Flagged'], ['11', '29', '24', 'United States Gary Bettenhausen', '209.756', '193', 'Flagged'], ['12', '20', '8', 'Australia Geoff Brabham', '207.082', '193', 'Flagged'], ['13', '22', '22', 'Brazil Raul Boesel', '211.202', '192', 'Flagged'], ['14', '33', '23', 'United States Dick Simon', '204.978', '189', 'Flagged'], ['15', '19', '61', 'Netherlands Arie Luyendyk', '207.811', '188', 'Crash T4'], ['16', '14', '15', 'United States Pancho Carter', '209.635', '179', 'Wheel Bearing'], ['17', '10', '66', 'United States Ed Pimm', '210.874', '168', 'Electrical'], ['18', '17', '55', 'Mexico Josele Garza', '208.939', '167', 'Flagged'], ['19', '32', '9', 'Brazil Roberto Moreno (R)', '209.469', '158', 'Stalled'], ['20', '15', '81', 'Canada Jacques Villeneuve (R)', '209.397', '154', 'Main Bearing'], ['21', '25', '59', 'United States Chip Ganassi', '207.590', '151', 'Engine'], ['22', '5', '11', 'United States Al Unser (W)', '212.295', '149', 'Vibration'], ['23', '16', '25', 'United States Danny Ongais', '209.158', '136', 'Ignition'], ['24', '21', '14', 'United States A. J. Foyt (W)', '213.212', '135', 'Spun in pits'], ['25', '27', '6', 'United States Rich Vogler', '209.089', '132', 'Crash T3'], ['26', '31', '84', 'United States George Snider', '209.025', '110', 'Ignition'], ['27', '28', '95', 'United States Johnny Parsons', '207.894', '100', 'CV Joint'], ['28', '18', '16', 'United States Tony Bettenhausen, Jr.', '208.933', '77', 'Valve Spring'], ['29', '26', '31', 'United Kingdom Jim Crawford', '208.911', '70', 'Head Gasket'], ['30', '23', '71', 'United States Scott Brayton', '208.079', '69', 'Engine'], ['31', '24', '42', 'United States Phil Krueger (R)', '207.948', '67', 'Engine'], ['32', '30', '2', 'United States Mario Andretti (W)', '212.300', '19', 'Ignition'], ['33', '7', '33', 'United States Tom Sneva (W)', '211.878', '0', 'Crash T2']], 'highlighted_cell_ids': [[6, 3], [6, 4], [32, 3], [32, 4]], 'question': 'How did Michael and Mario Andretti do?', 'answer': 'Michael Andretti finished with a run of 214.522 mph, faster than Mario.'}\nLine 3 (parsed): {'feta_id': 11734, 'table_source_json': 'totto_source/train_json/example-4033.json', 'page_wikipedia_url': 'http://en.wikipedia.org/wiki/List_of_best-selling_albums_in_Japan', 'table_page_title': 'List of best-selling albums in Japan', 'table_section_title': 'List of best-selling albums by domestic acts', 'table_array': [['No.', 'Album', 'Artist', 'Released', 'Chart', 'Sales'], ['1', 'First Love', 'Hikaru Utada', '10 March 1999', '1', '7,672,000'], ['2', 'B\\'z The Best \"Pleasure\"', \"B'z\", '20 May 1998', '1', '5,136,000'], ['3', 'Review', 'Glay', '1 October 1997', '1', '4,876,000'], ['4', 'Distance', 'Hikaru Utada', '28 March 2001', '1', '4,472,000'], ['5', 'B\\'z The Best \"Treasure\"', \"B'z\", '20 September 1998', '1', '4,439,000'], ['6', 'A Best', 'Ayumi Hamasaki', '28 March 2001', '1', '4,312,000'], ['7', 'Globe', 'Globe', '31 March 1996', '1', '4,136,000'], ['8', 'Deep River', 'Hikaru Utada', '19 June 2002', '1', '3,605,000'], ['9', 'Umi no Yeah!!', 'Southern All Stars', '25 June 1998', '1', '3,592,000'], ['10', 'Delicious Way', 'Mai Kuraki', '28 June 2000', '1', '3,530,000'], ['11', 'Time to Destination', 'Every Little Thing', '15 April 1998', '1', '3,520,000'], ['12', 'Atomic Heart', 'Mr. Children', '1 September 1994', '1', '3,430,000'], ['13', 'Sweet 19 Blues', 'Namie Amuro', '22 July 1996', '1', '3,359,000'], ['14', 'Bolero', 'Mr. Children', '5 March 1997', '1', '3,283,000'], ['15', 'Neue Musik', 'Yumi Matsutoya', '6 November 1998', '1', '3,252,000'], ['16', 'Faces Places', 'Globe', '12 March 1997', '1', '3,239,000'], ['17', 'The Swinging Star', 'Dreams Come True', '14 November 1992', '1', '3,227,000'], ['18', 'Impressions', 'Mariya Takeuchi', '25 July 1994', '1', '3,067,000'], ['19', 'Zard Best the Single Collection ～軌跡～', 'Zard', '28 May 1999', '1', '3,034,000'], ['20', 'All Singles Best', 'Kobukuro', '27 September 2006', '1', '3,018,000']], 'highlighted_cell_ids': [[1, 1], [1, 2], [1, 3], [2, 1], [2, 3], [2, 5]], 'question': 'How many copies did \"Pleasure\" sell in 1998 alone, and how long was it the best selling album in Japan?', 'answer': 'B\\'z The Best \"Pleasure\" sold more than 5 million copies in 1998 alone, making it a temporary best-selling album in Japanese music history, until being surpassed by Utada Hikaru\\'s First Love in 1999.'}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3dc6800e3644b3d9c31d841d2736375"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating valid split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31de76324535489dbe799b41b1e5fed9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"201fbcf75eb04e8581fcf302b0ad661c"}},"metadata":{}},{"name":"stdout","text":"Sample train example: {'feta_id': 18162, 'table_source_json': 'totto_source/train_json/example-10461.json', 'page_wikipedia_url': 'http://en.wikipedia.org/wiki/1982_Illinois_gubernatorial_election', 'table_page_title': '1982 Illinois gubernatorial election', 'table_section_title': 'Results', 'table_array': [['Party', 'Party', 'Candidate', 'Votes', '%', '±'], ['-', 'Republican', 'James R. Thompson (incumbent)', '1,816,101', '49.44', '-'], ['-', 'Democratic', 'Adlai Stevenson III', '1,811,027', '49.30', '-'], ['-', 'Libertarian', 'Bea Armstrong', '24,417', '0.66', '-'], ['-', 'Taxpayers', 'John E. Roche', '22,001', '0.60', '-'], ['-', 'N/A', 'write-ins', '161', '0.00', 'n-a'], ['Majority', 'Majority', 'Majority', '5,074', '0.14', '-'], ['Turnout', 'Turnout', 'Turnout', '3,673,707', '-', '-'], ['-', 'Republican hold', 'Republican hold', 'Swing', '-', '-']], 'highlighted_cell_ids': [[1, 2], [6, 3]], 'question': 'Who won the 1982 Illinois gubernatorial election, and how many votes was the margin?', 'answer': 'Thompson prevailed in the 1982 Illinois gubernatorial election by a 5,074 vote margin.'}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/7326 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01e269365dc74129aef4de38867c86f6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1001 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2d6e988d42f4b0a8a90cda327e93bd0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2003 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99729529a8c74ea0b37f27ccadb3b1be"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4570' max='4570' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4570/4570 3:34:17, Epoch 9/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Sacrebleu</th>\n      <th>Rouge1</th>\n      <th>Rouge2</th>\n      <th>Rougel</th>\n      <th>Bertscore Precision</th>\n      <th>Bertscore Recall</th>\n      <th>Bertscore F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1000</td>\n      <td>1.585000</td>\n      <td>1.267573</td>\n      <td>56.551000</td>\n      <td>0.602100</td>\n      <td>0.378000</td>\n      <td>0.497500</td>\n      <td>0.936100</td>\n      <td>0.910200</td>\n      <td>0.922700</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>1.507600</td>\n      <td>1.267616</td>\n      <td>56.551000</td>\n      <td>0.602100</td>\n      <td>0.378100</td>\n      <td>0.497600</td>\n      <td>0.936100</td>\n      <td>0.910200</td>\n      <td>0.922700</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>1.542500</td>\n      <td>1.267655</td>\n      <td>56.551000</td>\n      <td>0.602000</td>\n      <td>0.378000</td>\n      <td>0.497400</td>\n      <td>0.936100</td>\n      <td>0.910200</td>\n      <td>0.922700</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.000000</td>\n      <td>nan</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.665000</td>\n      <td>0.724900</td>\n      <td>0.693500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4607edc2569f45efa35fe7351e42b128"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f4b427301fa47239bc49ba0f41f07ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1046b22d91ea431782b6afa0719123a3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"222020fcfeab44278e4c1ade1d88a895"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c29c80ad8a584215b6ab58c451103832"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\nXet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99d0f26ca66a4c678f2e27572ef2b606"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:232: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\nSome weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:232: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\nSome weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:232: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\nSome weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:232: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:232: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\nSome weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Test Results: {'test_loss': 1.229372501373291, 'test_sacrebleu': 33.8871, 'test_rouge1': 0.6096, 'test_rouge2': 0.3822, 'test_rougeL': 0.5028, 'test_bertscore_precision': 0.9359, 'test_bertscore_recall': 0.9115, 'test_bertscore_f1': 0.9233, 'test_runtime': 929.7958, 'test_samples_per_second': 2.154, 'test_steps_per_second': 1.078}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:232: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}