{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11608025,"sourceType":"datasetVersion","datasetId":7280920}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install required libraries with compatible versions\n!pip install torch==2.5.1 transformers==4.44.2 tokenizers==0.19.1 peft==0.12.0 datasets==2.21.0 sentencepiece==0.2.0 accelerate==0.34.2 nltk==3.9.1 rouge_score==0.1.2 bert-score==0.3.13 sacrebleu==2.4.3 numpy>=1.26.4 fsspec>=2024.6.1 spacy==3.7.5 --force-reinstall --no-cache-dir --quiet\n\n# Set environment variables\nimport os\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n# Import libraries\nimport json\nimport numpy as np\nimport pandas as pd\nimport torch\nimport nltk\nimport spacy\nfrom datasets import load_dataset\nfrom rouge_score import rouge_scorer\nfrom sacrebleu.metrics import BLEU\nfrom bert_score import score as bert_score\nfrom transformers import (\n    AutoConfig,\n    AutoModelForSeq2SeqLM,\n    AutoTokenizer,\n    DataCollatorForSeq2Seq,\n    Seq2SeqTrainer,\n    Seq2SeqTrainingArguments,\n    set_seed,\n    GenerationConfig,\n    __version__ as transformers_version\n)\nfrom peft import LoraConfig, get_peft_model, TaskType\nnltk.download(\"punkt\", quiet=True)\nnlp = spacy.load(\"en_core_web_sm\")\n\n# Verify library versions\nprint(f\"Installed transformers version: {transformers_version}\")\n\n# Set random seed for reproducibility\ndef seed_everything(seed=42):\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    set_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.backends.cudnn.deterministic = True\nseed_everything()\n\n# Initialize model and tokenizer\nmodel_name = \"t5-base\"\nconfig = AutoConfig.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name, model_max_length=512)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name, config=config)\nmodel.resize_token_embeddings(len(tokenizer))\n\n# Enable gradient checkpointing\nmodel.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": False})\n\n# Apply LoRA\nlora_config = LoraConfig(\n    task_type=TaskType.SEQ_2_SEQ_LM,\n    r=8,\n    lora_alpha=32,\n    lora_dropout=0.1,\n    target_modules=[\"q\", \"v\"]\n)\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()\n\n# Debug dataset\nprint(\"Available datasets in /kaggle/input/:\")\nprint(os.listdir('/kaggle/input/'))\nprint(\"Files in /kaggle/input/feta-dataset/:\")\nprint(os.listdir('/kaggle/input/feta-dataset/'))\n\n# Load FeTaQA dataset\ndataset = load_dataset('json', data_files={\n    'train': '/kaggle/input/feta-dataset/fetaQA-v1_train.jsonl',\n    'valid': '/kaggle/input/feta-dataset/fetaQA-v1_dev.jsonl',\n    'test': '/kaggle/input/feta-dataset/fetaQA-v1_test.jsonl'\n})\nprint(\"Sample train example:\", dataset['train'][0])\n\n# Question Parsing Module\ndef parse_question(question):\n    doc = nlp(question)\n    keywords = [token.text.lower() for token in doc if token.pos_ in [\"NOUN\", \"PROPN\", \"NUM\"]]\n    intent = \"aggregate\" if any(word in question.lower() for word in [\"total\", \"sum\", \"average\", \"count\"]) else \"lookup\"\n    return {\"keywords\": keywords, \"intent\": intent}\n\n# Information Retrieval Module\ndef retrieve_relevant_cells(example):\n    table_array = example['table_array']\n    question = example['question']\n    highlighted_cells = example['highlighted_cell_ids']\n    parsed = parse_question(question)\n    keywords = parsed[\"keywords\"]\n    intent = parsed[\"intent\"]\n    \n    header = table_array[0]\n    rows = table_array[1:]\n    \n    relevant_cols = [i for i, col in enumerate(header) if any(kw.lower() in str(col).lower() for kw in keywords)]\n    if not relevant_cols:\n        relevant_cols = [i for i, _ in enumerate(header)]\n    \n    relevant_rows = set()\n    for cell_id in highlighted_cells:\n        if cell_id[0] > 0:\n            relevant_rows.add(cell_id[0] - 1)\n    for i, row in enumerate(rows):\n        if any(any(kw.lower() in str(cell).lower() for kw in keywords) for cell in row):\n            relevant_rows.add(i)\n    \n    relevant_cells = [[row_idx + 1, col_idx] for row_idx in relevant_rows for col_idx in relevant_cols]\n    return relevant_cells, intent\n\n# Reasoning Module\ndef reason_over_cells(table_array, relevant_cells, intent):\n    if intent == \"aggregate\":\n        values = []\n        for cell in relevant_cells:\n            row, col = cell\n            try:\n                value = float(table_array[row][col])\n                values.append(value)\n            except (ValueError, TypeError):\n                continue\n        if values:\n            return f\"Aggregated value: {sum(values) / len(values) if 'average' in intent else sum(values)}\"\n    return None\n\n# Enhanced Table Linearization\ndef linearize_table_context(example):\n    table_array = example['table_array']\n    question = example['question']\n    highlighted_cells = example['highlighted_cell_ids']\n    parsed = parse_question(question)\n    intent = parsed[\"intent\"]\n    header = table_array[0]\n    rows = table_array[1:]\n    \n    highlighted_str = \" [HIGHLIGHTED_CELLS] \"\n    for cell_id in highlighted_cells:\n        row_idx, col_idx = cell_id\n        if row_idx == 0:\n            cell_value = header[col_idx]\n        else:\n            cell_value = rows[row_idx - 1][col_idx]\n        highlighted_str += f\"row {row_idx} column {col_idx}: {cell_value}, \"\n    highlighted_str = highlighted_str.rstrip(\", \")\n    \n    table_str = \" [TABLE] [HEADER] \" + \" | \".join(str(col) for col in header) + \" [ROWS] \"\n    for i, row in enumerate(rows):\n        row_str = \"[ROW] \"\n        for j, cell in enumerate(row):\n            col_name = header[j]\n            row_str += f\"{col_name}: {cell} [SEP] \"\n        row_str = row_str.rstrip(\" [SEP] \")\n        table_str += row_str + \" \"\n    \n    linearized = f\"[QUESTION] {question} [INTENT] {intent}{highlighted_str}{table_str}\"\n    \n    if intent == \"aggregate\":\n        relevant_cells, _ = retrieve_relevant_cells(example)\n        reasoning_output = reason_over_cells(table_array, relevant_cells, intent)\n        if reasoning_output:\n            linearized += f\" [REASONING] {reasoning_output}\"\n    \n    return linearized\n\n# Preprocess dataset\ndef preprocess_examples(examples):\n    prefix = 'answer: '\n    inputs = [prefix + linearize_table_context(example) for example in examples]\n    answers = [example['answer'] for example in examples]\n    \n    model_inputs = tokenizer(\n        inputs,\n        max_length=512,\n        padding=\"max_length\",\n        truncation=True,\n        return_tensors=\"pt\"\n    )\n    \n    labels = tokenizer(\n        answers,\n        max_length=64,\n        padding=\"max_length\",\n        truncation=True,\n        return_tensors=\"pt\"\n    ).input_ids\n    \n    labels = [[-100 if token == tokenizer.pad_token_id else token for token in label] for label in labels]\n    model_inputs[\"labels\"] = labels\n    return model_inputs\n\n# Apply preprocessing\ndef process_batch(batch):\n    keys = batch.keys()\n    batch = [{k: batch[k][i] for k in keys} for i in range(len(batch[list(keys)[0]]))]\n    return preprocess_examples(batch)\n\nencoded_train_ds = dataset['train'].map(process_batch, batched=True, remove_columns=dataset['train'].column_names)\nencoded_val_ds = dataset['valid'].map(process_batch, batched=True, remove_columns=dataset['valid'].column_names)\nencoded_test_ds = dataset['test'].map(process_batch, batched=True, remove_columns=dataset['test'].column_names)\n\n# Post-process text for evaluation\ndef postprocess_text(preds, labels, metric_name):\n    preds = [pred.strip() for pred in preds]\n    labels = [label.strip() for label in labels]\n    if metric_name == \"rouge\":\n        preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n        labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n    elif metric_name == \"sacrebleu\":\n        labels = [[label] for label in labels]\n    return preds, labels\n\n# Compute evaluation metrics manually\ndef compute_metrics(eval_preds):\n    preds, labels = eval_preds\n    if isinstance(preds, tuple):\n        preds = preds[0]\n    \n    preds = np.clip(preds, 0, tokenizer.vocab_size - 1)\n    \n    decoded_preds = []\n    for pred in preds:\n        try:\n            decoded = tokenizer.decode(pred, skip_special_tokens=True)\n            decoded_preds.append(decoded if decoded else \"<empty>\")\n        except Exception as e:\n            print(f\"Error decoding prediction: {e}\")\n            decoded_preds.append(\"<error>\")\n    \n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = [tokenizer.decode(label, skip_special_tokens=True) for label in labels]\n    \n    save_path = os.path.join(training_args.output_dir, \"predictions.json\")\n    with open(save_path, \"w\") as f:\n        json.dump({\"predictions\": decoded_preds, \"labels\": decoded_labels}, f, indent=4)\n    \n    result = {}\n    \n    try:\n        bleu = BLEU()\n        preds_bleu, labels_bleu = postprocess_text(decoded_preds, decoded_labels, \"sacrebleu\")\n        bleu_score = bleu.corpus_score(preds_bleu, labels_bleu).score\n        result[\"sacrebleu\"] = round(bleu_score, 4)\n    except Exception as e:\n        print(f\"Error computing sacrebleu: {e}\")\n        result[\"sacrebleu\"] = 0.0\n    \n    try:\n        scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n        preds_rouge, labels_rouge = postprocess_text(decoded_preds, decoded_labels, \"rouge\")\n        rouge_scores = {\"rouge1\": 0.0, \"rouge2\": 0.0, \"rougeL\": 0.0}\n        for pred, label in zip(preds_rouge, labels_rouge):\n            scores = scorer.score(label, pred)\n            for key in rouge_scores:\n                rouge_scores[key] += scores[key].fmeasure\n        for key in rouge_scores:\n            rouge_scores[key] /= len(preds_rouge)\n            result[key] = round(rouge_scores[key], 4)\n    except Exception as e:\n        print(f\"Error computing rouge: {e}\")\n        result.update({\"rouge1\": 0.0, \"rouge2\": 0.0, \"rougeL\": 0.0})\n    \n    try:\n        P, R, F1 = bert_score(decoded_preds, decoded_labels, lang=\"en\", model_type=\"roberta-base\", verbose=False)\n        result.update({\n            \"bertscore_precision\": round(P.mean().item(), 4),\n            \"bertscore_recall\": round(R.mean().item(), 4),\n            \"bertscore_f1\": round(F1.mean().item(), 4)\n        })\n    except Exception as e:\n        print(f\"Error computing bertscore: {e}\")\n        result.update({\"bertscore_precision\": 0.0, \"bertscore_recall\": 0.0, \"bertscore_f1\": 0.0})\n    \n    return result\n\n# Training arguments\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"/kaggle/working/\",\n    overwrite_output_dir=True,\n    do_train=True,\n    do_eval=True,\n    do_predict=True,\n    num_train_epochs=5,  # Reduced for faster training\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    gradient_accumulation_steps=8,\n    warmup_steps=500,\n    weight_decay=0.01,\n    learning_rate=2e-5,  # Lowered to prevent instability\n    max_grad_norm=1.0,   # Gradient clipping to prevent nan\n    optim=\"adamw_torch\",\n    predict_with_generate=True,\n    generation_max_length=64,\n    generation_num_beams=4,\n    eval_strategy=\"steps\",\n    eval_steps=500,      # More frequent evaluation\n    save_strategy=\"steps\",\n    save_steps=500,      # More frequent checkpoints\n    fp16=torch.cuda.is_available(),\n    logging_steps=250,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"bertscore_f1\",\n    report_to=\"none\",\n    gradient_checkpointing=True,\n    gradient_checkpointing_kwargs={\"use_reentrant\": False}\n)\n\n# Initialize data collator\ndata_collator = DataCollatorForSeq2Seq(\n    tokenizer,\n    model=model,\n    label_pad_token_id=-100\n)\n\n# Initialize trainer\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=encoded_train_ds,\n    eval_dataset=encoded_val_ds,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics\n)\n\n# Train the model\ntrainer.train()\n\n# Evaluate on test set\ntest_results = trainer.predict(encoded_test_ds)\nprint(\"Test Results:\", test_results.metrics)\n\n# Save the model\ntrainer.save_model(\"/kaggle/working/tableqa_model\")\n\n# Generate submission\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\nmodel.eval()\npredictions = []\nfor example in dataset['test']:\n    input_text = \"answer: \" + linearize_table_context(example)\n    inputs = tokenizer(input_text, max_length=512, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n    with torch.no_grad():\n        outputs = model.generate(\n            input_ids=inputs[\"input_ids\"],\n            attention_mask=inputs[\"attention_mask\"],\n            max_length=64,\n            num_beams=4,\n            early_stopping=True\n        )\n    pred = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    predictions.append(pred if pred else \"<empty>\")\nsubmission_df = pd.DataFrame({\n    \"question\": [example[\"question\"] for example in dataset['test']],\n    \"predicted_answer\": predictions,\n    \"true_answer\": [example[\"answer\"] for example in dataset['test']]\n})\nsubmission_df.to_csv(\"/kaggle/working/submission.csv\", index=False)\nprint(\"Submission saved to /kaggle/working/submission.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T16:40:28.305005Z","iopub.execute_input":"2025-04-30T16:40:28.305575Z"}},"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 20.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 20.0.0 which is incompatible.\nsigstore 3.6.1 requires rich~=13.0, but you have rich 14.0.0 which is incompatible.\nydata-profiling 4.16.1 requires matplotlib<=3.10,>=3.5, but you have matplotlib 3.10.1 which is incompatible.\nnilearn 0.11.1 requires scikit-learn>=1.4.0, but you have scikit-learn 1.2.2 which is incompatible.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.9.1 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.5, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\ngensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.15.2 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\ntensorflow-metadata 1.16.1 requires protobuf<6.0.0dev,>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\ngoogle-spark-connect 0.5.2 requires google-api-core>=2.19.1, but you have google-api-core 1.34.1 which is incompatible.\npandas-gbq 0.26.1 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.6.1 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 44.0.2 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.0.0 which is incompatible.\nimbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\nibis-framework 9.2.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nibis-framework 9.2.0 requires toolz<1,>=0.11, but you have toolz 1.0.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\ngoogle-cloud-bigtable 2.28.1 requires google-api-core[grpc]<3.0.0dev,>=2.16.0, but you have google-api-core 1.34.1 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\nlangchain-core 0.3.35 requires packaging<25,>=23.2, but you have packaging 25.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mInstalled transformers version: 4.44.2\ntrainable params: 884,736 || all params: 223,766,784 || trainable%: 0.3954\nAvailable datasets in /kaggle/input/:\n['feta-dataset']\nFiles in /kaggle/input/feta-dataset/:\n['fetaQA-v1_train.jsonl', 'fetaQA-v1_dev.jsonl', 'fetaQA-v1_test.jsonl']\nSample train example: {'feta_id': 18162, 'table_source_json': 'totto_source/train_json/example-10461.json', 'page_wikipedia_url': 'http://en.wikipedia.org/wiki/1982_Illinois_gubernatorial_election', 'table_page_title': '1982 Illinois gubernatorial election', 'table_section_title': 'Results', 'table_array': [['Party', 'Party', 'Candidate', 'Votes', '%', '±'], ['-', 'Republican', 'James R. Thompson (incumbent)', '1,816,101', '49.44', '-'], ['-', 'Democratic', 'Adlai Stevenson III', '1,811,027', '49.30', '-'], ['-', 'Libertarian', 'Bea Armstrong', '24,417', '0.66', '-'], ['-', 'Taxpayers', 'John E. Roche', '22,001', '0.60', '-'], ['-', 'N/A', 'write-ins', '161', '0.00', 'n-a'], ['Majority', 'Majority', 'Majority', '5,074', '0.14', '-'], ['Turnout', 'Turnout', 'Turnout', '3,673,707', '-', '-'], ['-', 'Republican hold', 'Republican hold', 'Swing', '-', '-']], 'highlighted_cell_ids': [[1, 2], [6, 3]], 'question': 'Who won the 1982 Illinois gubernatorial election, and how many votes was the margin?', 'answer': 'Thompson prevailed in the 1982 Illinois gubernatorial election by a 5,074 vote margin.'}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/7326 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea8050a9614847af8893f80a76b0e372"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1001 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"258d26661ad341c6b593110a8ecc077d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2003 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1589923e2ce49d8a06bcc0119119ac7"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2285' max='2285' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2285/2285 2:03:45, Epoch 4/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Sacrebleu</th>\n      <th>Rouge1</th>\n      <th>Rouge2</th>\n      <th>Rougel</th>\n      <th>Bertscore Precision</th>\n      <th>Bertscore Recall</th>\n      <th>Bertscore F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>1.953200</td>\n      <td>1.477901</td>\n      <td>48.327000</td>\n      <td>0.486400</td>\n      <td>0.291700</td>\n      <td>0.405300</td>\n      <td>0.908200</td>\n      <td>0.883300</td>\n      <td>0.895100</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>1.550300</td>\n      <td>1.273255</td>\n      <td>56.551000</td>\n      <td>0.603700</td>\n      <td>0.377100</td>\n      <td>0.496500</td>\n      <td>0.935800</td>\n      <td>0.909800</td>\n      <td>0.922400</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>1.514000</td>\n      <td>1.271583</td>\n      <td>56.551000</td>\n      <td>0.603600</td>\n      <td>0.377500</td>\n      <td>0.496500</td>\n      <td>0.935900</td>\n      <td>0.910100</td>\n      <td>0.922500</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>1.488500</td>\n      <td>1.271592</td>\n      <td>56.551000</td>\n      <td>0.603600</td>\n      <td>0.377500</td>\n      <td>0.496500</td>\n      <td>0.935800</td>\n      <td>0.910100</td>\n      <td>0.922500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\nSome weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:232: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\nSome weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:232: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\nSome weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:232: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\nSome weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:232: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:232: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\nSome weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Test Results: {'test_loss': 1.2327446937561035, 'test_sacrebleu': 31.4666, 'test_rouge1': 0.6122, 'test_rouge2': 0.3833, 'test_rougeL': 0.5038, 'test_bertscore_precision': 0.9358, 'test_bertscore_recall': 0.9118, 'test_bertscore_f1': 0.9234, 'test_runtime': 954.8764, 'test_samples_per_second': 2.098, 'test_steps_per_second': 1.049}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:232: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}