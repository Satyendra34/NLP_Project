{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11576572,"sourceType":"datasetVersion","datasetId":7258353}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install required libraries with compatible versions\n!pip install torch==2.5.1 transformers==4.44.2 tokenizers==0.19.1 peft==0.12.0 datasets==2.21.0 sentencepiece==0.2.0 accelerate==0.34.2 nltk==3.9.1 evaluate==0.4.3 rouge_score==0.1.2 bert-score==0.3.13 sacrebleu==2.4.3 numpy==1.26.4 fsspec==2024.6.1 spacy==3.7.6 --force-reinstall --no-cache-dir --quiet\n\n# Set environment variables\nimport os\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n# Import libraries\nimport json\nimport numpy as np\nimport torch\nimport nltk\nimport spacy\nfrom datasets import load_dataset\nimport evaluate\nfrom transformers import (\n    AutoConfig,\n    AutoModelForSeq2SeqLM,\n    AutoTokenizer,\n    DataCollatorForSeq2Seq,\n    Seq2SeqTrainer,\n    Seq2SeqTrainingArguments,\n    set_seed,\n    GenerationConfig\n)\nfrom peft import LoraConfig, get_peft_model, TaskType\nnltk.download(\"punkt\", quiet=True)\nnlp = spacy.load(\"en_core_web_sm\")\n\n# Set random seed for reproducibility\ndef seed_everything(seed=42):\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    set_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.backends.cudnn.deterministic = True\nseed_everything()\n\n# Initialize model and tokenizer\nmodel_name = \"t5-base\"\nconfig = AutoConfig.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name, model_max_length=512)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name, config=config)\nmodel.resize_token_embeddings(len(tokenizer))\n\n# Explicitly enable gradient checkpointing with use_reentrant=False\nmodel.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": False})\n\n# Apply LoRA for parameter efficiency\nlora_config = LoraConfig(\n    task_type=TaskType.SEQ_2_SEQ_LM,\n    r=8,\n    lora_alpha=32,\n    lora_dropout=0.1,\n    target_modules=[\"q\", \"v\"]\n)\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()\n\n# Debug dataset\nprint(\"Available datasets in /kaggle/input/:\")\nprint(os.listdir('/kaggle/input/'))\nprint(\"Files in /kaggle/input/feta-dataset/:\")\nprint(os.listdir('/kaggle/input/feta-dataset/'))\nwith open('/kaggle/input/feta-dataset/fetaQA-v1_train.jsonl', 'r') as f:\n    print(\"First 3 lines of fetaQA-v1_train.jsonl:\")\n    for i, line in enumerate(f):\n        if i < 3:\n            try:\n                parsed = json.loads(line.strip())\n                print(f\"Line {i+1} (parsed):\", parsed)\n            except json.JSONDecodeError as e:\n                print(f\"Line {i+1} (raw, failed to parse):\", line.strip(), f\"Error: {e}\")\n        else:\n            break\n\n# Load FeTaQA dataset\ndataset = load_dataset('json', data_files={\n    'train': '/kaggle/input/feta-dataset/fetaQA-v1_train.jsonl',\n    'valid': '/kaggle/input/feta-dataset/fetaQA-v1_dev.jsonl',\n    'test': '/kaggle/input/feta-dataset/fetaQA-v1_test.jsonl'\n})\nprint(\"Sample train example:\", dataset['train'][0])\n\n# Question Parsing Module\ndef parse_question(question):\n    doc = nlp(question)\n    keywords = [token.text.lower() for token in doc if token.pos_ in [\"NOUN\", \"PROPN\", \"NUM\"]]\n    intent = \"aggregate\" if any(word in question.lower() for word in [\"total\", \"sum\", \"average\", \"count\"]) else \"lookup\"\n    return {\"keywords\": keywords, \"intent\": intent}\n\n# Information Retrieval Module\ndef retrieve_relevant_cells(example):\n    table_array = example['table_array']\n    question = example['question']\n    highlighted_cells = example['highlighted_cell_ids']\n    parsed = parse_question(question)\n    keywords = parsed[\"keywords\"]\n    intent = parsed[\"intent\"]\n    \n    header = table_array[0]\n    rows = table_array[1:]\n    \n    relevant_cols = [i for i, col in enumerate(header) if any(kw.lower() in str(col).lower() for kw in keywords)]\n    if not relevant_cols:\n        relevant_cols = [i for i, _ in enumerate(header)]\n    \n    relevant_rows = set()\n    for cell_id in highlighted_cells:\n        if cell_id[0] > 0:\n            relevant_rows.add(cell_id[0] - 1)\n    for i, row in enumerate(rows):\n        if any(any(kw.lower() in str(cell).lower() for kw in keywords) for cell in row):\n            relevant_rows.add(i)\n    \n    relevant_cells = [[row_idx + 1, col_idx] for row_idx in relevant_rows for col_idx in relevant_cols]\n    return relevant_cells, intent\n\n# Reasoning Module\ndef reason_over_cells(table_array, relevant_cells, intent):\n    if intent == \"aggregate\":\n        values = []\n        for cell in relevant_cells:\n            row, col = cell\n            try:\n                value = float(table_array[row][col])\n                values.append(value)\n            except (ValueError, TypeError):\n                continue\n        if values:\n            return f\"Aggregated value: {sum(values) / len(values) if 'average' in intent else sum(values)}\"\n    return None\n\n# Enhanced Table Linearization\ndef linearize_table_context(example):\n    table_array = example['table_array']\n    question = example['question']\n    relevant_cells, intent = retrieve_relevant_cells(example)\n    \n    header = table_array[0]\n    linearized = f\"[QUESTION] {question} [INTENT] {intent} [TABLE] \"\n    linearized += \" | \".join(str(cell) for cell in header) + \" [ROWS] \"\n    \n    for i, row in enumerate(table_array[1:], 1):\n        row_str = []\n        for j, cell in enumerate(row):\n            if [i, j] in relevant_cells:\n                cell = f\"*{cell}*\"\n            row_str.append(str(cell))\n        linearized += \" | \".join(row_str) + \" [ROW] \"\n    \n    reasoning_output = reason_over_cells(table_array, relevant_cells, intent)\n    if reasoning_output:\n        linearized += f\"[REASONING] {reasoning_output} \"\n    \n    return linearized\n\n# Preprocess dataset\ndef preprocess_examples(examples):\n    prefix = 'answer: '\n    inputs = [prefix + linearize_table_context(example) for example in examples]\n    answers = [example['answer'] for example in examples]\n    \n    model_inputs = tokenizer(\n        inputs,\n        max_length=512,\n        padding=\"max_length\",\n        truncation=True,\n        return_tensors=\"pt\"\n    )\n    \n    labels = tokenizer(\n        answers,\n        max_length=64,\n        padding=\"max_length\",\n        truncation=True,\n        return_tensors=\"pt\"\n    ).input_ids\n    \n    labels = [[-100 if token == tokenizer.pad_token_id else token for token in label] for label in labels]\n    model_inputs[\"labels\"] = labels\n    return model_inputs\n\n# Apply preprocessing\ndef process_batch(batch):\n    keys = batch.keys()\n    batch = [{k: batch[k][i] for k in keys} for i in range(len(batch[list(keys)[0]]))]\n    return preprocess_examples(batch)\n\nencoded_train_ds = dataset['train'].map(process_batch, batched=True, remove_columns=dataset['train'].column_names)\nencoded_val_ds = dataset['valid'].map(process_batch, batched=True, remove_columns=dataset['valid'].column_names)\nencoded_test_ds = dataset['test'].map(process_batch, batched=True, remove_columns=dataset['test'].column_names)\n\n# Post-process text for evaluation\ndef postprocess_text(preds, labels, metric_name):\n    preds = [pred.strip() for pred in preds]\n    labels = [label.strip() for label in labels]\n    if metric_name == \"rouge\":\n        preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n        labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n    elif metric_name == \"sacrebleu\":\n        labels = [[label] for label in labels]\n    return preds, labels\n\n# Compute evaluation metrics with robust handling\ndef compute_metrics(eval_preds):\n    preds, labels = eval_preds\n    if isinstance(preds, tuple):\n        preds = preds[0]\n    \n    # Clip token IDs to valid range\n    preds = np.clip(preds, 0, tokenizer.vocab_size - 1)\n    \n    # Decode predictions with error handling\n    decoded_preds = []\n    for pred in preds:\n        try:\n            decoded = tokenizer.decode(pred, skip_special_tokens=True)\n            decoded_preds.append(decoded if decoded else \"<empty>\")\n        except Exception as e:\n            print(f\"Error decoding prediction: {e}\")\n            decoded_preds.append(\"<error>\")\n    \n    # Decode labels\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = [tokenizer.decode(label, skip_special_tokens=True) for label in labels]\n    \n    # Save predictions and labels for debugging\n    save_path = os.path.join(training_args.output_dir, \"predictions.json\")\n    with open(save_path, \"w\") as f:\n        json.dump({\"predictions\": decoded_preds, \"labels\": decoded_labels}, f, indent=4)\n    \n    result = {}\n    for metric_name in [\"sacrebleu\", \"rouge\", \"bertscore\"]:\n        metric = evaluate.load(metric_name)\n        decoded_preds_proc, decoded_labels_proc = postprocess_text(decoded_preds, decoded_labels, metric_name)\n        try:\n            if metric_name == \"rouge\":\n                res = metric.compute(predictions=decoded_preds_proc, references=decoded_labels_proc)\n                result.update({f\"rouge{k}\": round(v, 4) for k, v in res.items()})\n            elif metric_name == \"sacrebleu\":\n                res = metric.compute(predictions=decoded_preds_proc, references=decoded_labels_proc)\n                result[\"sacrebleu\"] = round(res[\"score\"], 4)\n            elif metric_name == \"bertscore\":\n                res = metric.compute(predictions=decoded_preds_proc, references=decoded_labels_proc, lang=\"en\", model_type=\"roberta-base\")\n                result.update({\n                    \"bertscore_precision\": round(np.mean(res[\"precision\"]), 4),\n                    \"bertscore_recall\": round(np.mean(res[\"recall\"]), 4),\n                    \"bertscore_f1\": round(np.mean(res[\"f1\"]), 4)\n                })\n        except Exception as e:\n            print(f\"Error computing {metric_name}: {e}\")\n            result[metric_name] = 0.0\n    \n    return result\n\n# Training arguments with robust generation config\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"/kaggle/working/\",\n    overwrite_output_dir=True,\n    do_train=True,\n    do_eval=True,\n    do_predict=True,\n    num_train_epochs=10,\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    gradient_accumulation_steps=8,\n    warmup_steps=500,\n    weight_decay=0.01,\n    learning_rate=3e-5,\n    optim=\"adamw_torch\",\n    predict_with_generate=True,\n    generation_max_length=64,\n    generation_num_beams=4,  # Use beam search for stable generation\n    eval_strategy=\"steps\",\n    eval_steps=1000,\n    save_strategy=\"steps\",\n    save_steps=1000,\n    fp16=torch.cuda.is_available(),\n    logging_steps=500,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"bertscore_f1\",\n    report_to=\"none\",\n    gradient_checkpointing=True,\n    gradient_checkpointing_kwargs={\"use_reentrant\": False}\n)\n\n# Initialize data collator\ndata_collator = DataCollatorForSeq2Seq(\n    tokenizer,\n    model=model,\n    label_pad_token_id=-100\n)\n\n# Initialize trainer\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=encoded_train_ds,\n    eval_dataset=encoded_val_ds,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics\n)\n\n# Train the model\ntrainer.train()\n\n# Evaluate on test set\ntest_results = trainer.predict(encoded_test_ds)\nprint(\"Test Results:\", test_results.metrics)\n\n# Save the model\ntrainer.save_model(\"/kaggle/working/tableqa_model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T09:51:23.936014Z","iopub.execute_input":"2025-04-28T09:51:23.936732Z","iopub.status.idle":"2025-04-28T14:12:44.529391Z","shell.execute_reply.started":"2025-04-28T09:51:23.936708Z","shell.execute_reply":"2025-04-28T14:12:44.528495Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Install required libraries with compatible versions\n!pip install torch==2.5.1 torchvision==0.20.1 transformers==4.44.2 tokenizers==0.19.1 peft==0.12.0 datasets==2.21.0 sentencepiece==0.2.0 accelerate==0.34.2 nltk==3.9.1 evaluate==0.4.3 rouge_score==0.1.2 bert-score==0.3.13 sacrebleu==2.4.3 numpy==1.26.4 fsspec==2024.6.1 --force-reinstall --no-cache-dir --quiet\n\n# Import libraries\nimport numpy as np\nimport torch\nimport os\nimport nltk\nimport json\nfrom datasets import load_dataset\nimport evaluate\nfrom transformers import (\n    AutoConfig,\n    AutoModelForSeq2SeqLM,\n    AutoTokenizer,\n    DataCollatorForSeq2Seq,\n    Seq2SeqTrainer,\n    Seq2SeqTrainingArguments,\n    set_seed\n)\n\nnltk.download(\"punkt\", quiet=True)\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n# Set random seed for reproducibility\ndef seed_everything(seed=42):\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    set_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.backends.cudnn.deterministic = True\n\nseed_everything()\n\n# Debug dataset path and file content\nprint(\"Available datasets in /kaggle/input/:\")\nprint(os.listdir('/kaggle/input/'))\nprint(\"Files in /kaggle/input/feta-dataset/:\")\nprint(os.listdir('/kaggle/input/feta-dataset/'))\n\n# Inspect first few lines of the JSONL file\nwith open('/kaggle/input/feta-dataset/fetaQA-v1_train.jsonl', 'r') as f:\n    print(\"First 3 lines of fetaQA-v1_train.jsonl:\")\n    for i, line in enumerate(f):\n        if i < 3:\n            try:\n                parsed = json.loads(line.strip())\n                print(f\"Line {i+1} (parsed):\", parsed)\n            except json.JSONDecodeError as e:\n                print(f\"Line {i+1} (raw, failed to parse):\", line.strip(), f\"Error: {e}\")\n        else:\n            break\n\n# Load FeTaQA dataset (JSONL format)\ndataset = load_dataset('json', data_files={\n    'train': '/kaggle/input/feta-dataset/fetaQA-v1_train.jsonl',\n    'valid': '/kaggle/input/feta-dataset/fetaQA-v1_dev.jsonl',\n    'test': '/kaggle/input/feta-dataset/fetaQA-v1_test.jsonl'\n})\n\n# Verify dataset structure\nprint(\"Sample train example:\", dataset['train'][0])\n\n# Initialize model and tokenizer\nmodel_name = \"t5-base\"\nconfig = AutoConfig.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name, model_max_length=1024)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name, config=config)\nmodel.resize_token_embeddings(len(tokenizer))\n\n# Enhanced table linearization\ndef linearize_table_context(example):\n    table_array = example['table_array']\n    question = example['question']\n    highlighted_cells = example['highlighted_cell_ids']\n    \n    # Create a header row\n    header = table_array[0]\n    linearized = f\"[QUESTION] {question} [TABLE] \"\n    \n    # Add headers\n    linearized += \" | \".join(str(cell) for cell in header) + \" [ROWS] \"\n    \n    # Add rows with highlighted cell emphasis\n    for i, row in enumerate(table_array[1:], 1):\n        row_str = []\n        for j, cell in enumerate(row):\n            if [i, j] in highlighted_cells:\n                cell = f\"*{cell}*\"  # Emphasize highlighted cells\n            row_str.append(str(cell))\n        linearized += \" | \".join(row_str) + \" [ROW] \"\n    \n    return linearized\n\n# Preprocess dataset\ndef preprocess_examples(examples):\n    prefix = 'answer: '\n    inputs = [prefix + linearize_table_context(example) for example in examples]\n    answers = [example['answer'] for example in examples]\n    \n    # Tokenize inputs\n    model_inputs = tokenizer(\n        inputs,\n        max_length=1024,\n        padding=\"max_length\",\n        truncation=True,\n        return_tensors=\"pt\"\n    )\n    \n    # Tokenize labels\n    labels = tokenizer(\n        answers,\n        max_length=128,\n        padding=\"max_length\",\n        truncation=True,\n        return_tensors=\"pt\"\n    ).input_ids\n    \n    # Replace padding token id with -100 for loss calculation\n    labels = [[-100 if token == tokenizer.pad_token_id else token for token in label] for label in labels]\n    \n    model_inputs[\"labels\"] = labels\n    return model_inputs\n\n# Apply preprocessing with explicit batch processing\ndef process_batch(batch):\n    # Debug batch input\n    print(\"Batch type:\", type(batch))\n    print(\"Batch keys:\", batch.keys())\n    \n    # Convert dictionary of lists to list of dictionaries\n    keys = batch.keys()\n    batch = [{k: batch[k][i] for k in keys} for i in range(len(batch[list(keys)[0]]))]\n    \n    # Debug first example in batch\n    print(\"First example in batch:\", batch[0], type(batch[0]))\n    \n    return preprocess_examples(batch)\n\nencoded_train_ds = dataset['train'].map(process_batch, batched=True, remove_columns=dataset['train'].column_names)\nencoded_val_ds = dataset['valid'].map(process_batch, batched=True, remove_columns=dataset['valid'].column_names)\nencoded_test_ds = dataset['test'].map(process_batch, batched=True, remove_columns=dataset['test'].column_names)\n\n# Post-process text for evaluation\ndef postprocess_text(preds, labels, metric_name):\n    preds = [pred.strip() for pred in preds]\n    labels = [label.strip() for label in labels]\n    \n    if metric_name == \"rouge\":\n        preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n        labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n    elif metric_name == \"sacrebleu\":\n        labels = [[label] for label in labels]\n    \n    return preds, labels\n\n# Compute evaluation metrics\ndef compute_metrics(eval_preds):\n    preds, labels = eval_preds\n    if isinstance(preds, tuple):\n        preds = preds[0]\n    \n    # Decode predictions and labels\n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n    \n    # Save predictions and labels\n    save_path = os.path.join(training_args.output_dir, \"predictions.json\")\n    with open(save_path, \"w\") as f:\n        json.dump({\"predictions\": decoded_preds, \"labels\": decoded_labels}, f, indent=4)\n    \n    result = {}\n    for metric_name in [\"sacrebleu\", \"rouge\", \"bertscore\"]:\n        metric = evaluate.load(metric_name)\n        decoded_preds_proc, decoded_labels_proc = postprocess_text(decoded_preds, decoded_labels, metric_name)\n        \n        if metric_name == \"rouge\":\n            res = metric.compute(predictions=decoded_preds_proc, references=decoded_labels_proc)\n            result.update({f\"rouge{k}\": round(v, 4) for k, v in res.items()})\n        elif metric_name == \"sacrebleu\":\n            res = metric.compute(predictions=decoded_preds_proc, references=decoded_labels_proc)\n            result[\"sacrebleu\"] = round(res[\"score\"], 4)\n        elif metric_name == \"bertscore\":\n            res = metric.compute(predictions=decoded_preds_proc, references=decoded_labels_proc, lang=\"en\")\n            result.update({\n                \"bertscore_precision\": round(np.mean(res[\"precision\"]), 4),\n                \"bertscore_recall\": round(np.mean(res[\"recall\"]), 4),\n                \"bertscore_f1\": round(np.mean(res[\"f1\"]), 4)\n            })\n    \n    # Calculate average generation length\n    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n    result[\"gen_len\"] = round(np.mean(prediction_lens), 4)\n    \n    return result\n\n# Training arguments\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"/kaggle/working/\",\n    overwrite_output_dir=True,\n    do_train=True,\n    do_eval=True,\n    do_predict=True,\n    num_train_epochs=10,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    gradient_accumulation_steps=4,\n    warmup_steps=500,\n    weight_decay=0.01,\n    learning_rate=3e-5,\n    optim=\"adamw_torch\",\n    predict_with_generate=True,\n    eval_strategy=\"steps\",\n    eval_steps=1000,\n    save_strategy=\"steps\",\n    save_steps=1000,\n    fp16=torch.cuda.is_available(),\n    logging_steps=500,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"bertscore_f1\",\n    report_to=\"none\"\n)\n\n# Initialize data collator\ndata_collator = DataCollatorForSeq2Seq(\n    tokenizer,\n    model=model,\n    label_pad_token_id=-100\n)\n\n# Initialize trainer\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=encoded_train_ds,\n    eval_dataset=encoded_val_ds,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics\n)\n\n# Train the model\ntrainer.train()\n\n# Evaluate on test set\ntest_results = trainer.predict(encoded_test_ds)\nprint(\"Test Results:\", test_results.metrics)\n\n# Save the model\ntrainer.save_model(\"/kaggle/working/tableqa_model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T13:12:09.529427Z","iopub.execute_input":"2025-04-27T13:12:09.530152Z","iopub.status.idle":"2025-04-27T17:00:01.684820Z","shell.execute_reply.started":"2025-04-27T13:12:09.530123Z","shell.execute_reply":"2025-04-27T17:00:01.683332Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Available datasets in /kaggle/input/:\")\nprint(os.listdir('/kaggle/input/'))\nprint(\"Files in /kaggle/input/feta-dataset/:\")\nprint(os.listdir('/kaggle/input/feta-dataset/'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T14:22:13.664237Z","iopub.execute_input":"2025-04-26T14:22:13.665071Z","iopub.status.idle":"2025-04-26T14:22:13.678726Z","shell.execute_reply.started":"2025-04-26T14:22:13.665040Z","shell.execute_reply":"2025-04-26T14:22:13.678152Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}